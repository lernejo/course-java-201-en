include::chapter-base.adoc[]
== Scaling & relative considerations

Some time ago, the habit of building monolithic applications with in-memory state (such applications are said _stateful_) needed more powerful servers to follow an increase in usage.

What needed to be increased:

* RAM (memory)
* CPU frequency
* Number of CPU cores
* Disk size

Such upgrades fall under *vertical scaling*.
However hardware prices are not proportional to power, but rather exponential.
This scaling type rapidly faces technical limit (maximum power of a CPU at a certain time) or financial limit.

Some systems, such as traditional SQL databases (Oracle, PostgreSQL, MySql, etc.) hav no choice but to use this kind of scaling.

Conversely, applications can be structured in a way that they can be scaled differently.

In this case, *horizontal scaling*, which involves adding more containers / VMs / serveurs to increase the system capacities in a *linear* way.

image::vertical_vs_horizontal_scaling.png[align=center]

Another downside of having a unique instance, is that any change needing a reboot (configuration, new version, etc.) will interrupt the service.

Whereas with several instances of the same service, rebooting one is transparent from the user perspective.

Rebooting all instances, each one at a time, is called *rolling update* and used to update an application without service interruption.

The only warning with such a system, is to pay attention to N-1 retro-compatibility, as during an update, both versions, the old one and the new one, will run behind the same load-balancer at some point.
So API of the old version must have the same contracts and behave the same when used on the new version.

=== Identify SPOFs


SPOFs (**S**ingle **P**oint **O**f **F**ailure), are parts of a system that cannot have more than one instance and by design lead to service interruption in case of a stop (crash, update, etc.).

It is important to have monitoring on these components in order to rapidly take action in case of an issue.

This reaction can be automated (switch to a replica, passive until then) or manual.

Knowing where SPOFs are, can also orient the system architecture to avoid flooding these components by setting up throttling or caching.

SQL databases are often SPOFs; but most of them have replication mechanisms capable of keeping several other instances up-to-date and ready to use if the main one fails.

.PostgreSQL replica switch mechanism (`pg_auto_failover`)
image::pg_auto_failover.png[align=center]

=== Identify points of contention

When a system is under heavy load, points of contention may appear.

We are talking about parts of the system with slower processing which defines the maximum speed of the overall system.

NOTE: Like with a stormwater run-off system, if one of the pipe has a low throughput, the whole system appears slow, risking congestion

A point of contention can induce cascading reactions or hide several other contentions.

To identify and improve them if necessary, a precise supervision of each components of the system is required.
Supervision consists, among other things, of a probe set which collect data periodically so they can be graphed.
These graphs reflect the system evolution over time and allow to make correlations between exogenous events and endogenous consequences.

=== Distribute the load
==== For synchronous communications

To distribute the load on multiple instances of a same service, we use load-balancers, whether they are hardware (F5, AltÃ©on, etc.) or software (HAProxy, Nginx, Traefik, etc.).

Multiple configurations are available, to weight target nodes or establish _sticky session_.

Software load-balancers are simpler to configure and can even be controlled through an API.
This last capability allow to automate operations such as the previously mentioned *rolling update*.

==== For asynchronous communications

When using asynchronous communication protocols, its is best to rely on a dedicated message broker which acts as a buffer between applications.

Several tools and mechanisms are available to make a broker _**H**igh **A**vailability_ and capable of ingesting large quantities of data.

Such a broker can ingest occasional pressure surges and damper them for listening applications.

When communicating asynchronously, using pull flow allow a listening application to consume events at its own pace.

Load distribution is available _by design_ by adding more instances of listening applications.

Most brokers offer different delivery strategies:

* *at most once*: in this case, events can be loss
** Use case: notifications, data of little importance
* *at least once*: in this case, events can be delivered more than once, listening applications must be able to behave in an idempotent way
** Use case: every other cases, where loosing data is not an option (customer order, bank transaction, etc.)

=== Architecture evolution example
Consider a _typical_ application such as:

.Context diagram
image::korekto_context.png[align=center]

.Container diagram
image::korekto_container_v0.png[align=center]

==== Introduction of a load-balancer

Horizontal scaling, is the ability to increase a system capacity by adding more instances (or nodes).
But from the users perspective, there is only one service (https://korekto.io for example).
To map from one service to multiple instances, we use a load-balancer.

Warning, the part that executes a task every 4 hours must only be active on one instance, under penalty of doing the same job multiple times, which can lead to:

* Pointless CPU load
* Incoherent data

image::korekto_container_v1.png[align=center]

==== Split the application

Some features are used less often and uses less CPU.
It can be interresting to split our monolithic application into smaller ones, to be able to update them independently and scale them differently.
This separation must respect one golden rule to be durable:

[IMPORTANT]
====
Applications must be independent technically AND logically (from a business perspective).

This means that if one application is to evolve, it can do so without impact to others.
====

image::korekto_container_v2.png[align=center]

==== Deleting SPOFs

**S**ingle **P**oint **O**f **F**ailure are components of the system that cannot be replicated.
This causes 2 issues:

* If the component stops working, it causes a service disruption
* If the traffic increases, the only possibility is vertical scaling

SPOF nature is often due to a state stored in memory or a low latency storage need.
This is the case of the *[batch]* component.

If multiple instances were to execute simultaneously, how could they synchronize to distribute the work and restart jobs of failing instances ?

In our case, the simplest move is to use GitHub webhooks to be notified in case of a change, rather than periodically scan for change.

image::korekto_container_v3.png[align=center]

==== Do not exchange data through storage

During the split, a mistake was made.
Indeed, the *[web]* container and the *[grader]* one exchange data through the database.

This is well known anti-pattern, because:

* How to change the database model without impact for both containers?
* How to change the way data is stored without impact for both containers?

These two containers are coupled.

The objective now, is to introduce a new container which will abstract the storage.

image::korekto_container_v4.png[align=center]

Therefore, when

* Some of the data must be stored in a different way (in a relational database for example)
[unstyled]
** -> the change occurs in one only container, the *[storage API]* one
* The exercise model evolves to include more data
[unstyled]
** -> *[storage API]* supplies a new API (GET / POST / PUT / DELETE) link:#[/api/admin/specification/exercise/${name}/v2]
** -> *[web]* may use this API so teachers can better define properties of an exercise
** -> *[grader]* may use this API to improve grading
** -> *[storage API]* may delete the old API (link:#[/api/admin/specification/exercise/${name}/v1]) when no container use it anymore

These kind of changes can be applied with no service disruption
