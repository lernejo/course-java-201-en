include::chapter-base.adoc[]
== Dimensionnement & considérations relatives

Historiquement, la construction d’application monolithique et dont l’état est géré en mémoire (_stateful_) nécessitait avec l’augmentation de la charge des serveurs plus puissants.

Il s’agissait donc d’augmenter

* RAM (mémoire)
* Fréquence CPU
* Nombre de coeur CPU
* Taille du disque dur

Dans ce cas, on parle de *dimensionnement vertical*.
Il se trouve que le coût du matériel n’est pas proportionnel à la puissance, mais plutôt exponentiel.
Ce type de dimensionnement (ou _scaling_) arrive donc rapidement à une limite technique (puissance maximale des CPUs à une époque, etc.) ou financière.

Certains systèmes, comme les bases de données SQL traditionnelles (Oracle, PostgreSQL, MariaDB, etc.) ne peuvent augmenter leurs capacités que de cette manière.

À l’inverse, les applications peuvent être structurées de manière à pouvoir être dimensionnées différemment.

On parle dans ce cas de *dimensionnement horizontal*, qui consiste à augmenter le nombre de conteneurs / vms / serveurs pour augmenter de manière linéaire le nombre d’appels que le système peut traiter.

image::vertical_vs_horizontal_scaling.png[align=center]

Un autre aspect négatif de l’approche instance unique est que tout changement nécessitant un redémarrage de l’application (configuration, nouvelle version, etc.) interrompt le service rendu.

Dans le cas où il y a plusieurs instances d’une même application, que ce soit derrière un *répartiteur de charge* (_load-balancer_) ou qu’elles consomment les messages d’un broker, en éteindre une ne coupe pas le service.

Il est tout à fait possible de configurer le *répartiteur de charge* afin de ne plus distribuer de connexion vers une instance, de laisser les connexions existantes se terminer et enfin de pouvoir éteindre l’instance sans impact pour les utilisateurs.

Traiter toutes les instances d’une même application comme celà pour réaliser une mise à jour est appelé *rolling update*.

Attention toutefois à ce que les systèmes connectés à l’application (clients d’une API, base de données, etc.) puissent fonctionner avec les deux versions qui vont cohabiter durant la mise à jour.

=== Identifier les SPOFs

Les SPOFs (**S**ingle **P**oint **O**f **F**ailure) sont les parties d’un système qui ne peuvent pas avoir plus d’une instance et qui par conséquent entrainent une coupure de service en cas d’arrêt (crash, mise à jour, etc.).

Il est important de les identifier et de monitorer leur état pour pouvoir réagir en cas de problème.

Cette réaction peut être automatique (bascule sur un réplica jusque-là passif) ou manuelle (démarrage de services alternatifs).

Tenir compte des SPOFs peut également influer sur l’architecture du système pour limiter au strict minimum la charge qui y est envoyée, ou mettre en place des mécanismes pour alléger cette charge (un cache de données par exemple).

Ces considérations sont applicables à tout système dont on voudrait diminuer l’usage (une API extérieure payante par exemple) et rejoigne de même concept de découplage (diminuer les dépendances entres composants).

Les bases de données (SQL) sont également souvent des SPOFs ; il est cependant possible dans la plupart des cas de mettre en place des réplicas pouvant remplacer la base principale en cas d’indisponibilité.

.Mécanisme de bascule pour PostgreSQL (`pg_auto_failover`)
image::pg_auto_failover.png[align=center]

=== Identifier les points de contention

Quand un système est soumis à de fortes charges, des points de contention (ou goulot d’étranglement) peuvent apparaître.

Il s’agit de partie de système dont la vitesse de traitement est plus lente et qui constitue la vitesse maximale du système dans son ensemble.

NOTE: À l’instar d’un système d’écoulement d’eaux pluviales, si un des tuyaux a un faible débit, c’est tout le système qui est plus lent, et qui risque l’engorgement

Ces points de contention peuvent provoquer des réactions en chaîne ou en cacher d’autres.

Pour les identifier et si nécessaire les améliorer, il est nécessaire d’avoir une supervision précise des différents composants du système.
La supervision est constituée entre autres, d’un ensemble de sondes qui permettent de relever des données à interval régulier et d’en faire des courbes.
Ces courbes témoignent de l’évolution du système dans le temps et permettent de faire la corrélation entre des évènements exogènes et des conséquences endogènes, notamment l’apparition de points de contention.

=== Répartir la charge
==== Pour les communications synchrones

Pour distribuer la charge sur plusieurs instances d’un même service, on utilise des répartiteurs de charge qu’ils soient matériel (F5, Altéon, etc.) ou logiciel (HAProxy, Nginx, Traefik, etc.).

Il existe différentes configurations possibles, notamment pour affecter des poids aux nœuds cibles ou établir une affinité de session (_sticky session_).

Les répartiteurs de charge logiciels sont plus simples à configurer et peuvent même être pilotés par une API.
Cela permet notamment d’automatiser des opérations telles que le *rolling update* précédemment mentionné.

==== Pour les communications asynchrones

Pour ce qui est des communications asynchrones, on va tendre le plus possible vers l’utilisation d’un _broker de message_ qui fera le tampon entre les applications communicantes.

Un certain nombre d’outils peuvent être mis en place pour rendre un broker _hautement disponible_ (**H**igh **A**vailability) et capable d’ingérer de grosses volumétries.

Ce faisant, le broker pourra accueillir les éventuels coups de béliers (très grosses charges temporaires) sans redistribuer la pression aux applications clientes.

En effet, quand deux systèmes communiquent, il est préférable de fonctionner en _flux tiré_ de sorte que l’application cliente consomme quand elle le veut et au rythme ou elle le peut les messages qui lui sont destinés.

La répartition de charge se fait naturellement avec l’augmentation du nombre d’instances de l’application cliente (pour Kafka, au sein d’un même *client-group*, le nombre de partitions doit être supérieure à celui des applications clientes).

Il faut cependant être vigilant à la stratégie de livraison de message :

* *at most once* : un message est délivré au plus une fois. Par conséquent, il peut y avoir des pertes de message (cas marginal).
** Cas d’usage : notifications, données peu importantes
* *at least once* : un message est délivré au moins une fois. Par conséquent, il peut être délivré plusieurs fois (cas marginal) et l’application cliente doit être capable de dé-dupliquer ces messages
** Cas d’usage : tout le reste, du moment qu’on ne peut pas se permettre de perdre un message (commande client, transaction bancaire, changement d’une fiche patient, etc.)

=== Exemple d’évolution d’architecture
Considérant une application _classique_ comme celle-ci :

.Diagramme de Contexte
image::korekto_context.png[align=center]

.Diagramme de Container
image::korekto_container_v0.png[align=center]

==== Introduction d’un répartiteur de charge

Le dimensionnement horizontal, c’est la capacité du système à répartir la charge sur plusieurs instances (ou nœuds).
Mais du point de vue des utilisateurs, il n’y a qu’un seul service (https://korekto.io par exemple).
Pour faire cela il est nécessaire d’introduire un répartiteur de charge.

Attention ici, la partie qui exécute une tâche toutes les 4 heures ne doit être active que sur une seule instance, sous peine de faire le même travail plusieurs fois, ce qui peut provoquer :

* Charge CPU inutile
* États incohérents en base

image::korekto_container_v1.png[align=center]

==== Découpage de l’applicatif

Certaines fonctions de l’application, sont exécutées plus au moins souvent, et utilisent plus ou moins de CPU.

Il peut être intéressant de découper une application monolithique en plusieurs petites applications pour pouvoir dimensionner finement les fonctions les plus sollicitées.

Ce découpage doit respecter une unique règle pour être profitable :

[IMPORTANT]
====
Les applications découpées à partir d’un monolithe doivent être indépendantes d’un point de vue métier et technique.

C’est-à-dire que si une de ces applications est amenée à évoluer, elle doit pouvoir être mise à jour sans impact dans les autres applications
====

image::korekto_container_v2.png[align=center]

==== Suppression des SPOFs

Les points de défaillance unique (**S**ingle **P**oint **O**f **F**ailure), sont des conteneurs du système qui ne peuvent pas être répliqués.
Cela pose 2 problèmes :

* Si le conteneur ne fonctionne plus, il y a une coupure de service
* Si le trafic augmente (ici le nombre d’exercices à corriger), la seule solution est par définition le dimensionnement vertical.

La cause des SPOFs est souvent un état en mémoire qui ne peut être facilement répliqué.
C’est le cas de notre conteneur *[batch]*.

En effet, si plusieurs instances devaient s’exécuter, comment pourraient-elles simplement se synchroniser pour se répartir le travail et reprendre celui des autres instances dans le cas de pannes ?

Dans notre cas, il est possible de faire plus simple en utilisant les hooks de GitHub pour être notifié en cas de changement, plutôt que de lancer une tache quoi qu’il arrive.

image::korekto_container_v3.png[align=center]

==== Ne pas communiquer par le stockage !

Quand nous avons découpé notre applicatif, nous avons commis une grave erreur.
En effet, le conteneur *[web]* et le conteneur *[grader]* communiquent par la base de données.

C’est un *anti-pattern* d’architecture, car :

* Comment changer la base par une autre sans impacter les 2 conteneurs ?
* Comment changer la façon de stocker la donnée sans impacter les 2 conteneurs ?

Bref, ces 2 conteneurs sont couplés.

Le but ici va être d’introduire un nouveau _container_ qui va abstraire le stockage.

image::korekto_container_v4.png[align=center]

Ainsi, dans le cas où

* Une partie de la donnée doit être stockée de manière différente (dans une base de donnée relationnelle par ex)
[unstyled]
** -> la seule modification à apporter est dans le code du conteneur *[storage API]*.
* Le format de spécification d’exercice évolue pour inclure plus de données --> [storage API] on propose une nouvelle API (GET / POST / PUT / DELETE) link:#[/api/admin/specification/exercise/${name}/v2]
[unstyled]
** -> *[web]* on utilise cette nouvelle API pour que les professeurs puissent écrire des spécifications plus détaillées
** -> *[grader]* on utilise cette nouvelle API pour corriger les exercices plus finement
** -> *[storage API]* on supprime l’ancienne API (link:#[/api/admin/specification/exercise/${name}/v1]) maintenant que plus aucun conteneur ne l’utilise

Ce genre de changement peut être apporté sans coupure de service.
